{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14196912-9f57-4692-8f82-8b40b66375c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47939b8e-3f6f-4031-a59a-0c0a58b36b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d6f6ed-14ba-4423-89d3-749705f8ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
    "\n",
    "TRAINING_FILENAMES = '../datasets/gazetrack_tfrec/train.tfrec'\n",
    "VALID_FILENAMES = '../datasets/gazetrack_tfrec/val.tfrec'\n",
    "TEST_FILENAMES = '../datasets/gazetrack_tfrec/test.tfrec'\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "SEED = tf.Variable(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d54977-561b-4eb5-8618-8350ac898e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"path\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"device\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"screen_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"screen_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_valid\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"dot_xcam\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_y_cam\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_x_pix\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_y_pix\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"reye_x1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_x2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b9bb79-2ce0-42fc-a77f-a649a1fefbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(image, training = True):\n",
    "    if training:\n",
    "        aug = tf.keras.Sequential([\n",
    "                layers.Resizing(128+10, 128+10),\n",
    "                layers.RandomCrop(128, 128, 256),\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Normalization(mean = (0.3741, 0.4076, 0.5425), variance = (0.0004, 0.0004, 0.0004))\n",
    "                ])\n",
    "        \n",
    "    else:\n",
    "        aug = tf.keras.Sequential([\n",
    "                layers.Resizing(128+10, 128+10),\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Normalization(mean = (0.3741, 0.4076, 0.5425), variance = (0.0004, 0.0004, 0.0004))\n",
    "                ])\n",
    "    \n",
    "    image = aug(image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f1785b8-3dc7-4da6-97f6-5cb2c50d3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(features):\n",
    "    image = features['image']\n",
    "    w = tf.shape(image)[0]\n",
    "    h = tf.shape(image)[1]\n",
    "    \n",
    "    w = tf.cast(w, tf.int64)\n",
    "    h = tf.cast(h, tf.int64)\n",
    "    \n",
    "    screen_w, screen_h = features['screen_w'], features['screen_h']\n",
    "    \n",
    "    kps = [features['leye_x1']/w, features['leye_y1']/h, features['leye_x2']/w, features['leye_y2']/h,\n",
    "           features['reye_x1']/w, features['reye_y1']/h, features['reye_x2']/w, features['reye_y2']/h]\n",
    "    # kps has type float64\n",
    "    \n",
    "\n",
    "    lx, ly, lw, lh = features['leye_x'], features['leye_y'], features['leye_w'], features['leye_h']\n",
    "    rx, ry, rw, rh = features['reye_x'], features['reye_y'], features['reye_w'], features['reye_h']\n",
    "    \n",
    "    # lx, ly, lw, lh = tf.cast((lx, ly, lw, lh), tf.int32)\n",
    "    # rx, ry, rw, rh = tf.cast((rx, ry, rw, rh), tf.int32)\n",
    "    \n",
    "    lx = tf.cast(lx, tf.int32)\n",
    "    ly = tf.cast(ly, tf.int32)\n",
    "    lw = tf.cast(lw, tf.int32)\n",
    "    lh = tf.cast(lh, tf.int32)\n",
    "    \n",
    "    rx = tf.cast(rx, tf.int32)\n",
    "    ry = tf.cast(ry, tf.int32)\n",
    "    rw = tf.cast(rw, tf.int32)\n",
    "    rh = tf.cast(rh, tf.int32)\n",
    "    \n",
    "    # l_eye = tf.image.crop_to_bounding_box(image, max(0, ly), max(0, lx), max(0, lh), max(0, lw))  \n",
    "    # r_eye = tf.image.crop_to_bounding_box(image, max(0, ry), max(0, rx), max(0, rh), max(0, rw))\n",
    "    \n",
    "    l_eye = tf.image.crop_to_bounding_box(image, ly, lx, lh, lw)  \n",
    "    r_eye = tf.image.crop_to_bounding_box(image, ry, rx, rh, rw)\n",
    "    \n",
    "    l_eye = tf.image.flip_left_right(l_eye)\n",
    "    \n",
    "    out = [features['dot_xcam'], features['dot_y_cam']]\n",
    "    # out has type float32\n",
    "    \n",
    "    l_eye = augmentation(l_eye)\n",
    "    r_eye = augmentation(r_eye)\n",
    "    \n",
    "    \n",
    "    return l_eye, r_eye, kps, out, screen_w, screen_h\n",
    "    # return l_eye, r_eye, out, screen_w, screen_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b07f0836-330a-4cbb-8363-c8aac8e332d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def get_batched_dataset(filenames, batch_size):\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.deterministic = False  # disable order, increase speed\n",
    "    \n",
    "    dataset = (\n",
    "        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "        .with_options(option_no_order)\n",
    "        .map(parse_tfrecord_fn, num_parallel_calls=AUTO)\n",
    "        .map(prepare_sample, num_parallel_calls=AUTO)\n",
    "        .shuffle(batch_size*10)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(buffer_size=AUTO)\n",
    "    )\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e2adc84-ff47-465f-b848-8011d75d25af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of train samples: 398654\n",
      "No. of val samples: 43458\n",
      "No. of test samples: 59563\n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_batched_dataset(TRAINING_FILENAMES, BATCH_SIZE)\n",
    "valid_dataset = get_batched_dataset(VALID_FILENAMES, BATCH_SIZE)\n",
    "test_dataset = get_batched_dataset(TEST_FILENAMES, BATCH_SIZE)\n",
    "\n",
    "train_len = sum(1 for _ in tf.data.TFRecordDataset(TRAINING_FILENAMES))\n",
    "val_len = sum(1 for _ in tf.data.TFRecordDataset(VALID_FILENAMES))\n",
    "test_len = sum(1 for _ in tf.data.TFRecordDataset(TEST_FILENAMES))\n",
    "\n",
    "print(f\"No. of train samples: {train_len}\")\n",
    "print(f\"No. of val samples: {val_len}\")\n",
    "print(f\"No. of test samples: {test_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b3fa1-36ee-4095-b1f8-d5a515b13c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0f3ab-44fc-4662-ad07-21f02d60d100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa3997-c7ae-4189-a8c0-49a14f82dd09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2066f5d-dae3-4890-bad7-64a673ab5620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
