{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0691b6c1-5c00-4b31-8875-cdbe6db9a6cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d2adf-b72e-4a95-a83f-a2e6d7c95a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, layers\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os \n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c72652-daf1-4f6a-81e3-1e289a4f1e18",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TFrec file & batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e1b04d-2cac-4ac8-a073-4357afe369e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = ['00503','01046','01231','01816','01866','02015','02152','02459','03004','03253']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9b489-f645-4c08-8b20-257307df6d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
    "\n",
    "TEST_FILENAMES = '../../datasets/Users_google_tfrec/'+ IDs[9]+'.tfrec'\n",
    "# TEST_FILENAMES = '../../datasets/mit_split_tfrec/test.tfrec'\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "SEED = tf.Variable(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b9548-5d81-4374-8f16-e0ca7b78f65c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parse TFRec fn & Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43c24c-a8fd-4b8e-963e-4690d21418f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"path\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"device\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"screen_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"screen_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_valid\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"face_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_x\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_w\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_h\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"dot_xcam\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_y_cam\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_x_pix\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"dot_y_pix\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        \"reye_x1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_x2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"reye_y2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_x2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"leye_y2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "    return example\n",
    "\n",
    "\n",
    "def augmentation(image, training = True):\n",
    "    if training:\n",
    "        aug = tf.keras.Sequential([\n",
    "                layers.Resizing(128+10, 128+10),\n",
    "                layers.RandomCrop(128, 128, 256),\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Normalization(mean = (0.3741, 0.4076, 0.5425), variance = (0.0004, 0.0004, 0.0004))\n",
    "                ])\n",
    "        \n",
    "    else:\n",
    "        aug = tf.keras.Sequential([\n",
    "                layers.Resizing(128, 128),\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Normalization(mean = (0.3741, 0.4076, 0.5425), variance = (0.0004, 0.0004, 0.0004))\n",
    "                ])\n",
    "    \n",
    "    image = aug(image)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def prepare_eval_sample(features):\n",
    "    img_feat = features['image']\n",
    "\n",
    "    h = tf.shape(img_feat)[0]\n",
    "    w = tf.shape(img_feat)[1]\n",
    "\n",
    "    w = tf.cast(w, tf.int64)\n",
    "    h = tf.cast(h, tf.int64)\n",
    "\n",
    "    screen_w, screen_h = features['screen_w'], features['screen_h']\n",
    "\n",
    "    kps = [features['leye_x1']/w, features['leye_y1']/h, features['leye_x2']/w, features['leye_y2']/h,\n",
    "           features['reye_x1']/w, features['reye_y1']/h, features['reye_x2']/w, features['reye_y2']/h]\n",
    "    # kps has type float64\n",
    "\n",
    "    lx, ly, lw, lh = int(features['leye_x']), int(features['leye_y']), int(features['leye_w']), int(features['leye_h'])\n",
    "    rx, ry, rw, rh = int(features['reye_x']), int(features['reye_y']), int(features['reye_w']), int(features['reye_h'])\n",
    "\n",
    "    lx = tf.clip_by_value(lx, 0, int(w)-lw)\n",
    "    ly = tf.clip_by_value(ly, 0, int(h)-lh)\n",
    "\n",
    "    rx = tf.clip_by_value(rx, 0, int(w)-rw)\n",
    "    ry = tf.clip_by_value(ry, 0, int(h)-rh)\n",
    "\n",
    "    l_eye = tf.image.crop_to_bounding_box(img_feat, ly, lx, lh, lw)\n",
    "    r_eye = tf.image.crop_to_bounding_box(img_feat, ry, rx, rh, rw)\n",
    "\n",
    "    l_eye = tf.image.flip_left_right(l_eye)\n",
    "\n",
    "    l_eye = augmentation(l_eye, False)\n",
    "    r_eye = augmentation(r_eye, False)\n",
    "\n",
    "    y = [features['dot_xcam'], features['dot_y_cam']]\n",
    "    # y has type float32\n",
    "    \n",
    "    path = features['path']\n",
    "\n",
    "    return (l_eye, r_eye, kps), y, path\n",
    "\n",
    "def get_eval_dataset(filenames, batch_size):\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.deterministic = False  # disable order, increase speed\n",
    "    \n",
    "    dataset = (\n",
    "        tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "        .with_options(option_no_order)\n",
    "        .map(parse_tfrecord_fn, num_parallel_calls=AUTO)\n",
    "        .map(prepare_eval_sample, num_parallel_calls=AUTO)\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "    \n",
    "    dataset_len = sum(1 for _ in tf.data.TFRecordDataset(filenames))\n",
    "    print(f\"No. of samples: {dataset_len}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ef660-04fb-42e7-ba29-7d5ffa76d28e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96aafa7-ba8d-4077-8cb8-f934358b17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_dataset = get_eval_dataset(TEST_FILENAMES, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4aea5-ddb7-43e4-bf78-44bc91daff7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37cb1f5-39ac-42b5-af16-51eaa6b5d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model = tf.keras.models.load_model('../../checkpoints/new_mod/gs/rlrop100/epoch-99-vl-2.371.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4eb69-bd41-40c1-b183-b9b78548932b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b2719-844d-406e-9030-02ba0301dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b01acfd-616e-47d2-beac-5038361ce73c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Penultimate Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a9c16-e051-4475-81f6-021c03e7054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 're_lu_2'\n",
    "pen_model = Model(inputs=gs_model.input, \n",
    "                  outputs=gs_model.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1621fd37-c114-4c5b-a02c-af94662087ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pen_model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b128ee61-901d-4a36-83b4-ce2e055bdde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c272346-0803-4c26-8e87-01582ef2fcd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddf534-1254-4193-a971-af902e79d162",
   "metadata": {
    "tags": []
   },
   "source": [
    "### tostr fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845ea36-dd26-4ef5-9725-fa6c226158c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2str(path):\n",
    "    path_np = np.array(path)\n",
    "    path_str = np.array_str(path_np)\n",
    "    result_1 = path_str.split('images/', 1)[1]\n",
    "    image_id = result_1.split('.jpg', 1)[0]\n",
    "    return image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e359b3-d990-4966-9df4-50f9222b1214",
   "metadata": {},
   "source": [
    "### Get PU & GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c0446c-e917-4fc3-952d-0ccd78e7d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for x_test,y_test, path in individual_dataset:\n",
    "    image_id = tensor2str(path)\n",
    "    pred = pen_model.predict(x_test, verbose=0) \n",
    "    pred_np = np.array(pred)\n",
    "    target = np.array(y_test)\n",
    "    \n",
    "    data.append([image_id,[pred_np],[target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc479a57-e2a1-42d3-9b0d-f8c27fe862e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = np.unique(target, axis=0)\n",
    "len(pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac280a-9079-4f73-97bf-07f724d18ce4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Checking Results format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2668cd70-7e5d-4f3b-832f-42c1fea640db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd228f60-3b8d-4c9d-a72f-e564e13f185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[10][0])\n",
    "print(data[10][1])\n",
    "print(data[10][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2d972-6970-427d-9984-2f9b6d8adb91",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11989489-66bb-4526-8de2-a883ceddf2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data,columns=['Image ID','Penultimate_Output','GT_Value'])\n",
    "path_csv = '../../CSVs/google_all/'+IDs[9]+'.csv'\n",
    "df.to_csv(path_csv, index = False)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
